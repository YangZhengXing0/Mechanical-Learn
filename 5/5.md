## 第五章：线性回归算法

解决回归问题  
思想简单，实现容易  
许多强大的非线性模型的基础  
结果具有很好的可解析性  
蕴含机器学习中的很多重要思想

### 5-1 简单线性回归

```python
#目标找到a和b，使得 (y-ax+b)**2 尽可能小（最小化误差的平方）
#损失函数，效用函数：通过分析问题，确定问题的损失函数和效用函数
#通过最优化损失函数或效用函数，获得机器学习的模型

```

### 5-2 最小二乘法

```python
#通过最小化误差的平方和寻找数据的最佳函数匹配
```

### 5-3 简单线性回归的实现

```python
#Simple Linear Regression
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1.,2.,3.,4.,5.])
y = np.array([1.,3.,2.,3.,5.])
plt.scatter(x,y)
plt.axis([0,6,0,6])
plt.show()

x_mean = np.mean(x)
y_mean = np.mean(y)

num = 0.0
d = 0.0
for x_i , y_i in zip(x,y):
    num +=(x_i - x_mean) * (y_i - y_mean)
    d += (x_i - x_mean)**2

a = num / d
b = y_mean - a*x_mean

y_hat = a * x + b
plt.scatter(x,y)
plt.plot(x,y_hat,color='r')
plt.axis([0,6,0,6])
plt.show()


#SimpleLinearRegression class
%run D:\github\Mechanical-Learn\5\SimpleLinearRegression.py
reg1 = SimpleLinearRegression1()
reg1.fit(x,y)
reg1.predict(np.array([x_predict]))

y_hat1 = reg1.predict(x)
plt.scatter(x,y)
plt.plot(x,y_hat1,color='r')
plt.axis([0,6,0,6])
plt.show()

```

### 5-4 向量化

```python
#向量相乘
class SimpleLinearRegression2:
  def __init__(self):
    self.a_ = None
    self.b_ = None

  def fit(self, x_train, y_train):
    assert x_train.ndim == 1, \
      "Simple Linear Regerssor can only solve single feature training data."
    assert len(x_train) == len(y_train),\
      "the size of x_train must be equal to the size of y_train"

    x_mean = np.mean(x_train)
    y_mean = np.mean(x_train)

    #使用for 运算
    # for x_i , y_i in zip(x_train,y_train):
    #     num +=(x_i - x_mean) * (y_i - y_mean)
    #     d += (x_i - x_mean)**2

    #向量相乘
    num = (x_train - x_mean).dot(y_train-y_mean)
    d = (x_train - x_mean).dot(x_train - x_mean)

    self.a_ = num / d
    self.b_ = y_mean - self.a_*x_mean

    return self

  def predict(self, x_predict):
    assert x_predict.ndim == 1, \
      "Simple Linear Regerssor can only solve single feature training data."
    assert self.a_ is not None and self.b_ is not None, \
      "must fit before predict"
    return np.array([self._predict(x) for x in x_predict])

  def _predict(self, x_single):
    return self.a_ * x_single + self.b_
  def __repr__(self):
    return "SimpleLinearRegression2()"
```

### 5-5 衡量线性回归法的指标 MSE,RMS,MAE

```python
#均方误差MSE(Mean Squared Error)
#均方根误差MSE(Root Mean Squared Error)
#平均绝对误差(Mean Absolute Error)

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data[:,5]
y = boston.target

#数据集删除最大
x= x[y<50.0]
y= y[y<50.0]

plt.scatter(x,y)
plt.show()


from sklearn.model_selection import train_test_split
%run D:\github\Mechanical-Learn\5\SimpleLinearRegression.py
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=666)

reg = SimpleLinearRegression2()
reg.fit(x_train,y_train)

print(x_train.shape)
print(x_test.shape)
print(reg.a_)
print(reg.b_)


plt.scatter(x_train,y_train)
plt.plot(x_train,reg.predict(x_train),color='r')
plt.show()
y_predict = reg.predict(x_test)


#MSE
mse_test = np.sum((y_predict - y_test) ** 2) / len(y_test)
#RMSE
from math import sqrt
rmse_test = sqrt(mse_test)
#MAE
mae_test = np.sum(np.absolute(y_predict - y_test)) / len(y_test)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
mean_squared_error(y_test,y_predict)
mean_absolute_error(y_test,y_predict)

```

### 5-6 最好的衡量线性回归法的指标 R Squared

```python
# R**2 = 1 - RSS[(预测结果-真实结果)**2] / TSS[(均值结果-真实结果)**2] {Baseline model}
# R**2 越大越好
# R**2 < 0  ： 数据不存在任何的线性关系
# R**2 = 1 - MSE(y_predict, y) / Var(y)

#R Square
from sklearn.model_selection import train_test_split
%run D:\github\Mechanical-Learn\5\SimpleLinearRegression.py
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=666)

reg = SimpleLinearRegression2()
reg.fit(x_train,y_train)

print(x_train.shape)
print(x_test.shape)
print(reg.a_)
print(reg.b_)


plt.scatter(x_train,y_train)
plt.plot(x_train,reg.predict(x_train),color='r')
plt.show()
y_predict = reg.predict(x_test)
reg.score(x_test,y_test)

```

### 5-7 多元线性回归和正规方程解

```python
#理解多元线性回归 Normal Equation
#优点：不需要对数据做归一化处理
#问题：时间复杂度高
```

### 5-8 实现多元线性回归

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split

%run D:\github\Mechanical-Learn\5\LinearRegression.py

boston = datasets.load_boston()
X = boston.data
y = boston.target

X= X[y<50.0]
y= y[y<50.0]

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

reg = LinearRegression()
reg.fit_normal(x_train,y_train)
reg.coef_
reg.interception

reg.score(x_test, y_test)
```

### 5-9 使用 scikit-learn 解决回归问题

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X= X[y<50.0]
y= y[y<50.0]

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

reg = LinearRegression()
reg.fit(x_train,y_train)
reg.score(x_test, y_test)


#KNN Regressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
param_grid = [
    {
        "weights":["uniform"],
        "n_neighbors": [i for i in range(1,11)]
    },
    {
        "weights":["distance"],
        "n_neighbors": [i for i in range(1,11)],
        "p":[i for i in range(1,6)]
    },
]

knn_reg = KNeighborsRegressor()
grid_search = GridSearchCV(knn_reg,param_grid,n_jobs=-1 , verbose=1)
grid_search.fit(x_train, y_train)

grid_search.best_params_
grid_search.best_score_
grid_search.best_estimator_.score(x_test,y_test)


```

### 5-10 线性回归的可解性和更多思考

```python
# 评价回归算法：R Squared
# 线性回归：只能解决回归问题，对数据有假设，对数据有强解析性，正规方程解，时间复杂度高
# KNN算法：对数据没有假设
```
