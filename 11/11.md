## 第十一章：支撑向量机 SVM

### 11-1 什么是 SVM

```python
#11-1 什么是SVM Support Vector Machine 支撑向量机

#决策边界不唯一，不适定问题，
#泛化能力更加好，泛化能力直接放在算法内部
#最近距离尽可能大，相等

#SVM 尝试寻找一个最优的决策边界，距离两个类别的最近的样本最远， 最近的样本是支撑向量。2d ，最大margin
#SVM ，解决线性可分的问题

```

### 11-2 SVM 背后的最优化问题

```python
#推导SVM，min(||w|| **2)*0.5，有条件的最优化问题  Hard Margin
```

### 11-3 Soft Margin SVM

```python
#11-3 Soft Margin SVM
#容错范围 eta:(WX+b)>= 1 - eta
#正则化：L1正则（eta求和），L1正则（eta平方求和）
```

### 11-4 scikit-learn 中的 SVM

```python
#11-4 11-4 scikit-learn中的SVM

#要做数据标准化的处理
#SVM 处理维度数据尺度
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y<2,:2]
y= y[y<2]

plt.scatter(X[y==0,0],X[y==0,1],color='red')
plt.scatter(X[y==1,0],X[y==1,1],color='blue')
plt.show()
###
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()
standardScaler.fit(X)
x_standardScaler = standardScaler.transform(X)
###
from sklearn.svm import LinearSVC
svc = LinearSVC(C=1e9)
svc.fit(x_standardScaler,y)
###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)

plot_decision_boundary(svc, axis=[-3,3,-3,3])
plt.scatter(x_standardScaler[y==0,0],x_standardScaler[y==0,1])
plt.scatter(x_standardScaler[y==1,0],x_standardScaler[y==1,1])
plt.show()
###
#C越小容错空间越大，C越大容错空间越小
svc2 = LinearSVC(C=0.01)
svc2.fit(x_standardScaler,y)
plot_decision_boundary(svc2, axis=[-3,3,-3,3])
plt.scatter(x_standardScaler[y==0,0],x_standardScaler[y==0,1])
plt.scatter(x_standardScaler[y==1,0],x_standardScaler[y==1,1])
plt.show()
###

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)

    w = model.coef_[0]
    b = model.intercept_[0]
    # w0*x0 + w1*x1 + b = 0
    #=> x1 = -w0/w1 * x0 - b/w1
    plot_x = np.linspace(axis[0],axis[1],200)
    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]
    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]

    up_index = (up_y >= axis[2]) & (up_y <= axis[3])
    down_index = (down_y >= axis[2]) & (down_y <= axis[3])

    plt.plot(plot_x[up_index], up_y[up_index],color="black")
    plt.plot(plot_x[down_index], down_y[down_index],color="gray")
plot_svc_decision_boundary(svc, axis=[-3,3,-3,3])
plt.scatter(x_standardScaler[y==0,0],x_standardScaler[y==0,1])
plt.scatter(x_standardScaler[y==1,0],x_standardScaler[y==1,1])
plt.show()
###
```

### 11-5 SVM 中使用多项式特征和核函数

```python
#11-5 SVM中使用多项式特征和核函数
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
X, y = datasets.make_moons(noise=0.15,random_state=666)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()

from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
def PolynomialSVC(degree,C=0.1):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('linearSVC',LinearSVC(C=C))
    ])
poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)

###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)

plot_decision_boundary(poly_svc, axis=[-1.5,2.5,-1,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()

#使用多项式核函数的SVM
from sklearn.svm import SVC
def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('KernelSVC',SVC(kernel='poly',degree=degree,C=C))
    ])
polynomialKernelSVC = PolynomialKernelSVC(degree = 3)
polynomialKernelSVC.fit(X, y)

plot_decision_boundary(polynomialKernelSVC, axis=[-1.5,2.5,-1,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
###
```

### 11-6 到底什么是核函数

```python
#11-6 什么是核函数
#Kernel Function  使用核函数的技巧，运算量少，存储空间
#多项式核函数 K(x,y) = (x*y + 1)^2
```

### 11-7 RBF 核函数

```python
#11-7 高斯核函数 升维
# m*n 的数据映射成为 m*m 的数据
#RBF核函，将每一个样本点，映射到一个无穷维的特征空间
#多项式特征，依靠升维使得原本线性不可分的数据线性可分
#自然语音处理

import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-4,5,1)
y = np.array((x >= -2) & (x<=2), dtype='int')

#数据线性不可分
plt.scatter(x[y==0], [0]*len(x[y==0]))
plt.scatter(x[y==1], [0]*len(x[y==1]))
plt.show()

#高斯核函数
def gausian(x, l):
    gamma = 1.0
    return np.exp(-gamma * (x-l)**2)

l1, l2 = -1, 1
X_new = np.empty((len(x), 2))

for i , data in enumerate(x):
    X_new[i, 0] = gausian(data, l1)
    X_new[i, 1] = gausian(data, l2)
#数据线性可分
plt.scatter(X_new[y==0,0], X_new[y==0,1])
plt.scatter(X_new[y==1,0], X_new[y==1,1])
plt.show()
```

### 11-8 RBF 核函数中的 gamma

```python
#11-8 高斯核函数的gamma参数
#gamma 越大，高斯分布越窄，
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
###
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
def RBFKernelSVC(gamma=1.0):
    return Pipeline([
        ('sta_scaler',StandardScaler()),
        ('svc', SVC(kernel='rbf', gamma = gamma))
    ])

svc = RBFKernelSVC()
svc.fit(X,y)
###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)

plot_decision_boundary(svc, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
###过拟合
svc2 = RBFKernelSVC(gamma=100)
svc2.fit(X,y)
plot_decision_boundary(svc2, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()

###欠拟合
svc2 = RBFKernelSVC(gamma=0.1)
svc2.fit(X,y)
plot_decision_boundary(svc2, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
```

### 11-9 SVM 思想解决回归问题

```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
datas = datasets.load_boston()
X = datas.data
y = datas.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

from sklearn.svm import LinearSVR
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def StanderLinerSVR(epsilon=0.1):
    return Pipeline([
        ('sta_scaler',StandardScaler()),
        ('linerSVR', LinearSVR(epsilon = epsilon))
    ])
ssvr = StanderLinerSVR()
ssvr.fit(X_train, y_train)
ssvr.score(X_test,y_test)
```
