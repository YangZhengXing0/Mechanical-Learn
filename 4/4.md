## 第四章：最基础的分类算法-k近邻算法 KNN


### 4-1 k近邻算法基础
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets as ds

iris = ds.load_iris()


iris.data.shape
iris.feature_names
iris.target
iris.target.shape
iris.target_names

#原数据
X_train = iris.data[:,:2]
Y_train = iris.target

plt.scatter(X_train[Y_train==0,0], X_train[Y_train==0,1], color="red",marker="o")
plt.scatter(X_train[Y_train==1,0], X_train[Y_train==1,1], color="blue",marker="+")
plt.scatter(X_train[Y_train==2,0], X_train[Y_train==2,1], color="green",marker="*")

#例子
li = np.array([5.3,3.6])

plt.scatter(li[0],li[1],color='black')

plt.show()

#计算 例子和样本直接的距离 append 到 distances  欧拉距离
from math import sqrt
distances = [sqrt(np.sum((x_train - li)**2)) for x_train in X_train]

#距离的key
nearest = np.argsort(distances)
k = 6
#获取距离最短的数组 参数K
topK_y = [Y_train[i] for i in nearest[:k]]

#Counter
from collections import Counter

votes = Counter(topK_y)
#期望值
predict_y = votes.most_common(1)[0][0]
```

### 4-2 scikit-learn中的机器学习算法封装
```python
#kNN 不需要训练模型的算法  对KNN来说，训练集就是模型
import numpy as np
from math import sqrt
from collections import Counter

def KNN_classify(k,X_train, y_train , x):

    assert 1 <= k <= X_train.shape[0],"K must be valid"
    assert X_train.shape[0] == y_train.shape[0],\
        "the size of X_train must equal to the size of y_traiin"


    distances = [sqrt(np.sum((x_train-x)**2)) for x_train in X_train]
    nearest = np.argsort(distances)
    
    topK_y = [y_train[i] for i in nearest[:k]]
    votes = Counter(topK_y)
    return votes.most_common(1)[0][0]


from sklearn import datasets as ds
iris = ds.load_iris()

X_train = iris.data[:,:2]
y_train = iris.target
KNN_classify(6,X_train,y_train,[4.5,5.3])



```

### 4-3 训练数据集，测试数据集
```python



```
### 4-4 分类准确度
```python



```
### 4-5 超参数
```python



```
### 4-6 网格搜索与k近邻算法中更多超参数
```python



```
### 4-7 数据归一化
```python



```

### 4-8 scikit-learn中的Scaler
```python



```
### 4-9 更多有关k近邻算法的思考
```python



```