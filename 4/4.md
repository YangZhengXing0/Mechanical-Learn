## 第四章：最基础的分类算法-k近邻算法 KNN


### 4-1 k近邻算法基础
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets as ds

iris = ds.load_iris()


iris.data.shape
iris.feature_names
iris.target
iris.target.shape
iris.target_names

#原数据
X_train = iris.data[:,:2]
Y_train = iris.target

plt.scatter(X_train[Y_train==0,0], X_train[Y_train==0,1], color="red",marker="o")
plt.scatter(X_train[Y_train==1,0], X_train[Y_train==1,1], color="blue",marker="+")
plt.scatter(X_train[Y_train==2,0], X_train[Y_train==2,1], color="green",marker="*")

#例子
li = np.array([5.3,3.6])

plt.scatter(li[0],li[1],color='black')

plt.show()

#计算 例子和样本直接的距离 append 到 distances  欧拉距离
from math import sqrt
distances = [sqrt(np.sum((x_train - li)**2)) for x_train in X_train]

#距离的key
nearest = np.argsort(distances)
k = 6
#获取距离最短的数组 参数K
topK_y = [Y_train[i] for i in nearest[:k]]

#Counter
from collections import Counter

votes = Counter(topK_y)
#期望值
predict_y = votes.most_common(1)[0][0]
```

### 4-2 scikit-learn中的机器学习算法封装
```python
#kNN 不需要训练模型的算法  对KNN来说，训练集就是模型
import numpy as np
from math import sqrt
from collections import Counter

def KNN_classify(k,X_train, y_train , x):

    assert 1 <= k <= X_train.shape[0],"K must be valid"
    assert X_train.shape[0] == y_train.shape[0],\
        "the size of X_train must equal to the size of y_traiin"


    distances = [sqrt(np.sum((x_train-x)**2)) for x_train in X_train]
    nearest = np.argsort(distances)
    
    topK_y = [y_train[i] for i in nearest[:k]]
    votes = Counter(topK_y)
    return votes.most_common(1)[0][0]


from sklearn import datasets as ds
iris = ds.load_iris()

X_train = iris.data[:,:2]
y_train = iris.target
KNN_classify(6,X_train,y_train,[4.5,5.3])

```
```python
# 使用scikit-learn中的kNN
import numpy as np
from math import sqrt
from collections import Counter
from sklearn.neighbors import KNeighborsClassifier
from sklearn import datasets as ds
iris = ds.load_iris()

X_train = iris.data[:,:2]
y_train = iris.target

KNN_classifier = KNeighborsClassifier(n_neighbors=6)
#模型
KNN_classifier.fit(X_train,y_train)
#样例
X_predict = np.array([5.5, 1.6]).reshape(1,-1)

y_predict =  KNN_classifier.predict(X_predict)

y_predict[0]


```

```python
#重新整理kNN 代码
%run D:\github\Mechanical-Learn\4\KNN.py
knn_clf = KNNClassifier(k=6)
knn_clf.fit(X_train,y_train)
y_predict = knn_clf.predict(X_predict)
y_predict[0]

```



### 4-3 训练数据集，测试数据集
```python
#判断机器学习算法的性能  模型很差？ 真实环境难以拿到真实label
##数据集 ：分成训练（80%）和测试（20%）数据集 ，通过测试数据判断模型好坏  train test split
import numpy as np
def train_text_split(X, y, text_ratio = 0.2, seed=None):
  #参数判断
  assert X.shape[0] == y.shape[0],\
    "the size of S must ve equal to the size of y"
  assert 0.0 <= text_ratio <= 1.0,\
    "test_ration must be valid"

  #随机种子
  if seed:
    np.random.seed(seed)
  
  #train 80%  test 20%  split
  shuffle_indexes = np.random.permutation(len(X))
  test_ratio = 0.2
  test_size = int(len(X)*test_ratio)
  test_indexes = shuffle_indexes[:test_size]
  train_indexes = shuffle_indexes[test_size:]

  #Fancy Indexing
  X_train = X[train_indexes]
  y_train = y[train_indexes]

  X_test = X[test_indexes]
  y_test = y[test_indexes]

  return X_train, y_train, X_test, y_test


#测试模型  
#数据集   
from sklearn import datasets as ds
iris = ds.load_iris()
X = iris.data[:,:2]
y = iris.target 

#本地模型
%run D:\github\Mechanical-Learn\4\KNN.py
%run D:\github\Mechanical-Learn\4\module_selection.py
X_train, y_train, X_test, y_test = train_text_split(X, y)
    
my_knn_clf = KNNClassifier(k=3)

my_knn_clf.fit(X_train,y_train)

y_predict = my_knn_clf.predict(X_test)

sum(y_predict == y_test)/len(y_test)



#sklearn 中的 train_text_split 
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X_train,X_test,y_train, y_test = train_test_split(X, y, test_size=0.2)

print(X_train.shape)
print(y_train.shape)

print(X_test.shape)
print(y_test.shape)

my_knn_clf = KNeighborsClassifier(n_neighbors=3)

my_knn_clf.fit(X_train,y_train)

y_predict = my_knn_clf.predict(X_test)

sum(y_predict == y_test)/len(y_test)
```


### 4-4 分类准确度
```python

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets
digits = datasets.load_digits()
#print(digits.DESCR)
X  = digits.data
y = digits.target
some_digit = X[666]
some_digit_image = some_digit.reshape(8,8)
plt.imshow(some_digit_image,cmap=matplotlib.cm.binary)
plt.show()


## 本地文件  修改KNNClassifier
%run D:\github\Mechanical-Learn\4\module_selection.py
%run D:\github\Mechanical-Learn\4\KNN.py

X_train,X_test,y_train, y_test = train_test_split(X, y, 0.2)
my_knn_clf = KNNClassifier(k=3)
my_knn_clf.fit(X_train,y_train)
my_knn_clf.score(X_test,y_test)


##使用sklearn accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X_train,X_test,y_train, y_test = train_test_split(X, y, test_size=0.2)
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train,y_train)

y_predict = knn_clf.predict(X_test)
accuracy_score(y_test,y_predict)
#knn_clf.score(X_test,y_test)
```




### 4-5 超参数
```python
#超参数：在算法运行前需要决定的参数
#模型参数：算法过程中学习的参数

#KNN算法没有模型参数，k的超参数

#寻找好的超参数（领域知识，经验数值，实验搜索）

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#寻找最好的K
best_score = 0.0
best_k= -1
for k in range(1,11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train,y_train)
    score = knn_clf.score(X_test,y_test)
    
    if score > best_score:
        best_score = score
        best_k = k

print("best_k = ", best_k)
print("best_score = ", best_score)

#KNN算法 距离的权重，解决平票问题

#是否考虑距离
best_method = ""
best_score = 0.0
best_k= -1

for method in ["uniform",'distance']:
    for k in range(1,11):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights=method)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)

        if score > best_score:
            best_score = score
            best_k = k
            best_method = method

print("best_k = ", best_k)
print("best_score = ", best_score)
print("best_method = ", best_method)

#欧拉距离（1次方），曼哈顿距离（2次方），明可夫斯基距离（P次方）  获得超参数P
best_p = -1
best_method = "distance"
best_score = 0.0
best_k= -1

for p in range(1,6):    
    for k in range(1,11):
        knn_clf = KNeighborsClassifier(n_neighbors=k,weights=best_method,p=p)
        knn_clf.fit(X_train,y_train)
        score = knn_clf.score(X_test,y_test)

        if score > best_score:
            best_score = score
            best_k = k
            best_method = best_method
            best_p = p

print("best_k = ", best_k)
print("best_score = ", best_score)
print("best_method = ", best_method)
print("best_p = ", best_p)




```
### 4-6 网格搜索与k近邻算法中更多超参数
```python



```
### 4-7 数据归一化
```python



```

### 4-8 scikit-learn中的Scaler
```python



```
### 4-9 更多有关k近邻算法的思考
```python



```