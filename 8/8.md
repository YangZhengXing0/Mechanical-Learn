## 第八章：多项式回归与模型泛化

### 8-1 什么是多项式回归

```python
#模型泛化  升维
# y = ax^2 + bx + c  把 x^2 , x 分别看出两个特征， 使用线性回归拟合数据
import numpy as np
import matplotlib.pyplot as plt
###数据
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(X, y)
plt.show()
###
#线性回归
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()


##解决方案，添加一个特征 X**2
X2 = np.hstack([X, X**2])
X2.shape

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
### 多项式x前面的系数
lin_reg2.coef_
```

### 8-2 scikit-learn 中的多项式回归于 pipeline

```python
import numpy as np
import matplotlib.pyplot as plt

###
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(X, y)
plt.show()
###
from sklearn.preprocessing import PolynomialFeatures
#degree 多少次幂
poly = PolynomialFeatures(degree=2)
poly.fit(X)
x2 = poly.transform(X)
x2.shape #(100，3)
###
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(x2, y)
y_predict2 = lin_reg.predict(x2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

# X2多出一列特征 相乘， 多项式指数级增长
X = np.arange(1, 11).reshape(-1,2)
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
X2.shape#(5,2)



#Pipeline  增加多项式特征，数据归一化，线性回归
import numpy as np
import matplotlib.pyplot as plt


x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(X, y)
plt.show()
###
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
pipl = Pipeline([
    ("poly",PolynomialFeatures(degree=2)),
    ("std_scaler", StandardScaler()),
    ("lin_reg", LinearRegression()),
])

###
pipl.fit(X, y)
y_predict = pipl.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

```

### 8-3 过拟合与欠拟合

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(X, y)
plt.show()
###
###使用线性回归 欠拟合
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(X, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

mean_squared_error(y , y_predict)

###使用多项式回归
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degreen):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression()),
    ])

### degreen = 2
pipl = PolynomialRegression(2)
pipl.fit(X, y)
y_predict = pipl.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
mean_squared_error(y , y_predict)

### degreen = 10000  过拟合
pipl = PolynomialRegression(100)
pipl.fit(X, y)
y_predict = pipl.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
mean_squared_error(y , y_predict)

```

### 8-4 为什么要训练数据集与测试数据集

```python
#过拟合与欠拟合 怎么解决 使用 训练数据集与测试数据集
import numpy as np
import matplotlib.pyplot as plt


x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
plt.scatter(X, y)
plt.show()
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

###
pipl = PolynomialRegression(2)
pipl.fit(x_train, y_train)
y_predict = pipl.predict(x_test)
mean_squared_error(y_test , y_predict)

###
pipl = PolynomialRegression(100)
pipl.fit(x_train, y_train)
y_predict = pipl.predict(x_test)
mean_squared_error(y_test , y_predict)

### 模型复杂度 和 模型准确率
# 训练数据集 ，模型复杂度 和 模型准确率   成线性关系
# 测试数据集 ，模型复杂度 和 模型准确率   有过拟合与欠拟合
#欠拟合，不能完整表达表达数据关系
#过拟合，过多表达数据间的噪音关系
# 网格搜索：寻找泛化能力最好的地方



```

### 8-5 学习曲线

```python
import numpy as np
import matplotlib.pyplot as plt


x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)

y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(X, y)
plt.show()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=666)
from sklearn.linear_model import LinearRegression
def plot_learning_curve(algo, x_train, x_test, y_train, y_test):
    import matplotlib.pyplot as plt
    from sklearn.metrics import mean_squared_error
    train_score = []
    test_score = []
    for i in range(1,len(x_train)+1):
        algo.fit(x_train[:i], y_train[:i])
        y_train_predict = algo.predict(x_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))
        y_test_predict = lin.predict(x_test[:i])
        test_score.append(mean_squared_error(y_test[:i], y_test_predict))

    plt.plot([i for i in range(1, len(x_train)+1)], np.sqrt(train_score),label="train")
    plt.plot([i for i in range(1, len(x_train)+1)], np.sqrt(test_score),label="test")
    plt.legend()
    plt.show()
plot_learning_curve(LinearRegression(), x_train, x_test, y_train, y_test)

###
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degreen):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression()),
    ])

plot_learning_curve(PolynomialRegression(2), x_train, x_test, y_train, y_test)


```

### 8-6 验证数据集与交叉验证

```python
#把数据分成3份：训练，测试，验证 数据集
#训练 ---> 生成模型的数据集
#验证 调整超参数使用的数据集
#测试 作为衡量最终模型性能的数据集

#交叉验证 （验证数据集过拟合）
#训练，分成 A,B,C 数据集， 3个模型（A , B , C）


#交叉验证 获取超参数
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

def cross(x_train, x_test, y_train, y_test):
    best_score,best_p,best_k=0,0,0
    for k in range(2,11):
        for p in range(1,6):
            knn = KNeighborsClassifier(weights='distance', n_neighbors=k,p=p)
            score = np.mean(cross_val_score(knn,x_train, y_train))
            if score > best_score:
                best_score,best_p,best_k=score,p,k
    print(best_score)
    print(best_p)
    print(best_k)

%time cross(x_train, x_test, y_train, y_test)

###网格搜索 就有交叉验证
param_grid = [
    {
        'weights':['distance'],
        'n_neighbors':[i for i in range(1,11)],
        'p':[i for i in range(2,6)]
    }
]
knn_clf = KNeighborsClassifier()
grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1,verbose=2)
grid_search.fit(x_train, y_train)

###
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(x_test, y_test)

### k-folds 交叉验证
### 留一法 LOO-CV 把训练数据集分成M份， 计算量大

```

### 8-7 偏差方差平衡

```python
#方差，数据是否分散；主要原因：模型太复杂，如高阶多项式回归，过拟合
#偏差，数据不分散，和目标值距离远；主要原因：问题本身的假设不正确，如非线性数据，使用线性回归，欠拟合
#模型误差：偏差，方差，不可避免的误差（数据噪音）

#高方差的算法：KNN，非参数学习
#高偏差的算法：线性回归，参数学习

#算法具有相应的参数，可以调整偏差和方差
#KNN  中的 k
#线性回归 中的 多项式
#方差，偏差通常是矛盾的

#机器学习的主要挑战，来自方差
#解决手段：降低模型的复杂度；减少数据维度，降噪；增加样本数；使用验证集；模型正则化
```

### 8-8 模型泛化与岭回归

```python

#模型正则化 Regularization， 限制参数的大小

import numpy as np
import matplotlib.pyplot as plt

np.random.seed = 666

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)

y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=666)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
def PolynomialRegression(degreen):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", lin_reg),
    ])

from sklearn.metrics import mean_squared_error
poly100 = PolynomialRegression(degreen=25)
poly100.fit(x_train,y_train)

y100_predict = poly100.predict(x_test)
print(mean_squared_error(y_test , y100_predict))

def plot_model(model):
    X_plot = np.linspace(-3, 3,100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3,3,0,10])
    plt.show()

plot_model(poly100)
### 使用岭回归
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1)

def RidgeRegression(degreen,alpha):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("ridge_reg", Ridge(alpha=alpha))
    ])
ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(x_train,y_train)

y101_predict = ridge1_reg.predict(x_test)
print(mean_squared_error(y_test , y101_predict))

plot_model(ridge1_reg)

###
ridge2_reg = RidgeRegression(20, 10)
ridge2_reg.fit(x_train,y_train)

y101_predict = ridge2_reg.predict(x_test)
print(mean_squared_error(y_test , y101_predict))

plot_model(ridge2_reg)


```

### 8-9 LASSO

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed = 666

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1,1)

y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=666)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
def PolynomialRegression(degreen):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", lin_reg),
    ])

from sklearn.metrics import mean_squared_error
poly100 = PolynomialRegression(degreen=25)
poly100.fit(x_train,y_train)

y100_predict = poly100.predict(x_test)
print(mean_squared_error(y_test , y100_predict))

def plot_model(model):
    X_plot = np.linspace(-3, 3,100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3,3,0,10])
    plt.show()

plot_model(poly100)

###
#Lasso 趋向于使得一部分theta值变为0，所以可作为特征选择用， 阶段下降(-1, -1),(0，-1)，Ridge是走曲线
from sklearn.linear_model import Lasso
ridge = Ridge(alpha=1)

def LassoRegression(degreen,alpha):
    return Pipeline([
        ("poly",PolynomialFeatures(degree=degreen)),
        ("std_scaler", StandardScaler()),
        ("ridge_reg", Lasso(alpha=alpha))
    ])
lasso_reg = LassoRegression(20, 0.01)
lasso_reg.fit(x_train,y_train)

y101_predict = lasso_reg.predict(x_test)
print(mean_squared_error(y_test , y101_predict))

plot_model(lasso_reg)


```

### 8-10 L1,L2 和弹性网络

```python
# L1  L2 正则
#Ridge  L2正则项
#LASSO  L1正则项

#弹性网：结合 Ridge + LASSO，先尝试Ridge
[弹性网络](https://blog.csdn.net/xgxyxs/article/details/79436219)
from itertools import cycle
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import lasso_path, enet_path
from sklearn import datasets

diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target

X /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)

# Compute paths

eps = 5e-3  # the smaller it is the longer is the path

print("Computing regularization path using the lasso...")
alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)

print("Computing regularization path using the positive lasso...")
alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(
    X, y, eps, positive=True, fit_intercept=False)
print("Computing regularization path using the elastic net...")
alphas_enet, coefs_enet, _ = enet_path(
    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)

print("Computing regularization path using the positive elastic net...")
alphas_positive_enet, coefs_positive_enet, _ = enet_path(
    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)

# Display results

plt.figure(1)
ax = plt.gca()

colors = cycle(['b', 'r', 'g', 'c', 'k'])
neg_log_alphas_lasso = -np.log10(alphas_lasso)
neg_log_alphas_enet = -np.log10(alphas_enet)
for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):
    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and Elastic-Net Paths')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')
plt.axis('tight')


plt.figure(2)
ax = plt.gca()
neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)
for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):
    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso and positive Lasso')
plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')
plt.axis('tight')


plt.figure(3)
ax = plt.gca()
neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)
for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):
    l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)
    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)

plt.xlabel('-Log(alpha)')
plt.ylabel('coefficients')
plt.title('Elastic-Net and positive Elastic-Net')
plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),
           loc='lower left')
plt.axis('tight')
plt.show()
```
