## 第 7 章 PCA 与梯度上升法

### 7-1 什么是 PCA

```python
#主成分分析
#非监督学习
#降维，可视化，去噪

#把数据映射到一个轴上，使样本间的方差最大
#如何找到让样本间距最大的轴
#如何定义样本间间距
#使用方差 （Variance）

#样本均值归为0  把坐标轴移动 （简化方程式）
```

### 7-2 使用梯度上升法求解 PCA 问题

```python
#推导方程式
# 2*X^^T(Xw) / m
```

### 7-3 求数据的主成分 PCA

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100,2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)


def demean(X):
    return X-np.mean(X,axis=0)
#第一步：demean
X_demean = demean(X)
plt.scatter(X_demean[:,0], X_demean[:,1])
plt.show()

#梯度上升法
def f(w, X):
    return np.sum((X.dot(w)**2)) / len(X)

def df_math(w, X):
    return X.T.dot(X.dot(w)) * 2. / len(X)

def df_debug(w, X,epsilon=0.0001):
    res = np.empty(len(w))
    for i in range(len(w)):
        w_1 = w.copy()
        w_1[i] += epsilon
        w_2 = w.copy()
        w_2[i] -= epsilon
        res[i] = (f(w_1, X) - f(w_2, X)) / (2*epsilon)
    return res

def direction(w):
    return w / np.linalg.norm(w)

def gradient_ascent(df, X_b,initial_w, eta, n_inters=1e4, epsilon=1e-8):
    w = direction(initial_w)
    i_inter = 0
    while i_inter < n_inters:
        gradient = df(w, X_b)
        last_w = w
        w = w + eta * gradient
        w = direction(w)  #注意1： 每次求一个单位方向
        i_inter += 1
        if(abs(f(w, X_b) - f(last_w, X_b)) < epsilon):
            break
    return w

initial_w = np.random.random(X.shape[1]) #注意2 不能用0向量开始
eta = 0.001
#注意3 不能 使用standardScaler 标准化数据（方差不能为1）

print(gradient_ascent(df_debug,X_demean, initial_w, eta))
print(gradient_ascent(df_math,X_demean, initial_w, eta))


w = gradient_ascent(df_math,X_demean, initial_w, eta)
plt.scatter(X_demean[:,0], X_demean[:,1])
plt.plot([0, w[0]*30],[0, w[1]*30],color='r')
plt.show()


#不加噪
X2 = np.empty((100,2))
X2[:,0] = np.random.uniform(0., 100., size=100)
#X2[:,1] = 0.75 * X2[:,0] + 3. + np.random.normal(0, 10., size=100)
#不加噪
X2[:,1] = 0.75 * X2[:,0] + 3.

X2_demean = demean(X2)
w2 = gradient_ascent(df_math,X2_demean, initial_w, eta)
plt.scatter(X2_demean[:,0], X2_demean[:,1])
plt.plot([0, w2[0]*30],[0, w2[1]*30],color='r')
plt.show()
```

### 7-4 求数据的前 n 个主成分

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100,2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

def demean(X):
    return X-np.mean(X,axis=0)
#第一步：demean
X_demean = demean(X)
plt.scatter(X_demean[:,0], X_demean[:,1])
plt.show()


def f(w, X):
    return np.sum((X.dot(w)**2)) / len(X)

def df_math(w, X):
    return X.T.dot(X.dot(w)) * 2. / len(X)

def df_debug(w, X,epsilon=0.0001):
    res = np.empty(len(w))
    for i in range(len(w)):
        w_1 = w.copy()
        w_1[i] += epsilon
        w_2 = w.copy()
        w_2[i] -= epsilon
        res[i] = (f(w_1, X) - f(w_2, X)) / (2*epsilon)
    return res

def direction(w):
    return w / np.linalg.norm(w)
#第一个成分
def first_component(X_b,initial_w, eta, n_inters=1e4, epsilon=1e-8):
    w = direction(initial_w)
    i_inter = 0
    while i_inter < n_inters:
        gradient = df_math(w, X_b)
        last_w = w
        w = w + eta * gradient
        w = direction(w)  #注意1： 每次求一个单位方向
        i_inter += 1
        if(abs(f(w, X_b) - f(last_w, X_b)) < epsilon):
            break
    return w
###
initial_w = np.random.random(X.shape[1]) #注意2 不能用0向量开始
eta = 0.001
w = first_component(X, initial_w, eta)
w
###
X2 = np.empty(X.shape)
for i in range(len(X)):
    X2[i] = X[i] - X[i].dot(w)*w

plt.scatter(X2[:,0], X2[:,1])
plt.show()
###
w2 = first_component(X2,initial_w,eta)
#趋近于零
w.dot(w2)


#前n个主成分
def first_n_components(n, X,eta=0.01, n_inters=1e4, epsilon=1e-8):
    X_pca = X.copy()
    X_pca = demean(X_pca)
    res = []
    for i in range(n):
        initial_w = np.random.random(X_pca.shape[1])
        w = first_component(X_pca, initial_w, eta)
        res.append(w)
        X_pca = X_pca - X_pca.dot(w).reshape(-1, 1) * w

    return res
###
first_n_components(2,X)
```

### 7-5 高维数据映射为低维数据

```python
%run D:\github\Mechanical-Learn\7\PCA.py
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100,2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 10., size=100)

###
pca = PCA(n_components=2)
pca.fit(X)
pca.components_

###
pca = PCA(n_components=1)
pca.fit(X)
pca.components_
#降维
X_reduction = pca.transform(X)
X_reduction.shape #1

#恢复数据
X_restore = pca.inverse_transform(X_reduction)
X_restore.shape #2

plt.scatter(X[:,0], X[:,1],color='b',alpha=0.5)
plt.scatter(X_restore[:,0], X_restore[:,1],color='r',alpha=0.5)
plt.show()
```

### 7-6 scikit-learn 中的 PCA

```python
###使用PCA 降维处理 1：提高运算效率 2：可视化数据（降到2维）
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
# 查看时间
%time knn_clf.fit(x_train, y_train)
knn_clf.score(x_test, y_test)

###PCA 降维处理后的计算时间
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(x_train)
#降维处理
x_train_reduction = pca.transform(x_train)
x_test_reduction = pca.transform(x_test)

%time knn_clf.fit(x_train_reduction, y_train)
#精度损失
knn_clf.score(x_test_reduction, y_test)


#解释方差比例
pca.explained_variance_ratio_
pca = PCA(n_components=x_train.shape[1])
pca.fit(x_train)
#方向轴对应重要程度
pca.explained_variance_ratio_


### 直接传精度
pca = PCA(0.95)
pca.fit(x_train)
pca.n_components_

x_train_reduction = pca.transform(x_train)
x_test_reduction = pca.transform(x_test)
#查看时间 比之前少
%time knn_clf.fit(x_train_reduction, y_train)

```

### 7-7 试手 MNIST 数据集

####MNIST 数据集  
[mnist-original](https://raw.githubusercontent.com/amplab/datascience-sp14/master/lab7/mldata/mnist-original.mat)

```python

#Machina_Learn_data/mldata
minst = fetch_mldata("MNIST original",data_home='./Machina_Learn_data')
print(minst)

import numpy as np
from sklearn.datasets import fetch_mldata
minst = fetch_mldata("MNIST original",data_home='./Machina_Learn_data')
X, y = minst['data'], minst['target']
X_train = np.array(X[:60000],dtype=float)
y_train = np.array(y[:60000],dtype=float)
X_test = np.array(X[60000:],dtype=float)
y_test = np.array(y[60000:],dtype=float)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
# 查看时间
%time knn_clf.fit(X_train, y_train)
%time knn_clf.score(X_test, y_test)

#PCA降维处理
from sklearn.decomposition import PCA
pca = PCA(0.9)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)
# 查看时间
%time knn_clf.fit(X_train_reduction, y_train)
%time knn_clf.score(X_test_reduction, y_test)


```

### 7-8 使用 PCA 对数据进行降噪

```python

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
#噪音
noisy_digits = X + np.random.normal(0, 4, size=X.shape)

####
example_digits = noisy_digits[y==0,:][:10]
print(example_digits.shape)
for num in range(1, 10):
    X_num = noisy_digits[y==num,:][:10]
    example_digits = np.vstack([example_digits, X_num])

def plot_digits(data):
    fig, axes = plt.subplots(10,10,figsize=(10,10),subplot_kw={'xticks':[],'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i , ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8,8),
                 cmap='binary',interpolation='nearest',clim=(0,16))
plot_digits(example_digits)
plt.show()

####
pca = PCA(0.5)
pca.fit(noisy_digits)
#降维处理
components = pca.transform(example_digits)
filter_digits = pca.inverse_transform(components)
plot_digits(filter_digits)
plt.show()


```

### 7-9 人脸识别与特征脸

```python
#特征脸 高维数据向低维数据映射
###
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn import datasets
digits = datasets.load_digits()
X = digits.data
y = digits.target
###
noisy_digits = X + np.random.normal(0, 4, size=X.shape)
example_digits = noisy_digits[y==0,:][:10]
print(example_digits.shape)
for num in range(1, 10):
    X_num = noisy_digits[y==num,:][:10]
    example_digits = np.vstack([example_digits, X_num])

def plot_digits(data):
    fig, axes = plt.subplots(10,10,figsize=(10,10),subplot_kw={'xticks':[],'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i , ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8,8),
                 cmap='binary',interpolation='nearest',clim=(0,16))
plot_digits(example_digits)
plt.show()
###
pca = PCA(0.5)
pca.fit(noisy_digits)
#降维处理
components = pca.transform(example_digits)
filter_digits = pca.inverse_transform(components)
plot_digits(filter_digits)
plt.show()


###
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people()
random_indexes = np.random.permutation(len(faces.data))
X = faces.data[random_indexes]
example_faces = X[:36,:]
example_faces.shape

def plot_faces(faces):
    fig, axes = plt.subplots(6,6,figsize=(10,10),subplot_kw={'xticks':[],'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i , ax in enumerate(axes.flat):
        ax.imshow(faces[i].reshape(62,47), cmap='bone')
    plt.show()
plot_faces(example_faces)
###
pca1 = PCA(svd_solver='randomized')
%time pca1.fit(X)
plot_faces(pca1.components_[:36,:])

```
