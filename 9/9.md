## 第九章：逻辑回归

### 9-1 什么是逻辑回归

```python
#Logistic Regresstion 解决分类问题
# p=f(x)   p<0.5 y=1; p>0.5 y=0;p回归和y分类
#Sigmoid 函数
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1/(1+np.exp(-t))


x = np.linspace(-10,10,500)
y = sigmoid(x)
plt.plot(x,y)
plt.show()

```

### 9-2 逻辑回归的损失函数

```python
#cost= -ylog(p) - (1-y)log(1-p)  损失函数 （y=1, y=0） 梯度下降法
```

### 9-3 逻辑回归损失函数的梯度

```python
#算法推导
```

### 9-4 实现逻辑回归算法

```python
%run D:\github\Mechanical-Learn\9\LogisticRegression.py
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)

log_reg.score(x_test, y_test)
log_reg.predict_proba(x_test)

```

### 9-5 决策边界

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
iris = datasets.load_iris()

X = iris.data
y = iris.target

x = X[y<2, :2]
y = y[y<2]



%run D:\github\Mechanical-Learn\9\LogisticRegression.py
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)

log_reg.score(x_test, y_test)
log_reg.predict_proba(x_test)


def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.intercept_) / log_reg.coef_[1]

x1_plot = np.linspace(4,8, 1000)
x2_plot = x2(x1_plot)

plt.plot(x1_plot,x2_plot)
plt.scatter(x[y==0,0],x[y==0,1], color='red')
plt.scatter(x[y==1,0],x[y==1,1], color='blue')
plt.show()

#不规则的决策边界绘制方法
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)

plot_decision_boundary(log_reg, axis=[4,7.5,1.5,4.5])
plt.scatter(x[y==0,0],x[y==0,1], color='red')
plt.scatter(x[y==1,0],x[y==1,1], color='blue')
plt.show()
###
#KNN 的决策边界
from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
knn_clf.fit(x_train,y_train)
plot_decision_boundary(knn_clf, axis=[4,7.5,1.5,4.5])
plt.scatter(x[y==0,0],x[y==0,1], color='red')
plt.scatter(x[y==1,0],x[y==1,1], color='blue')
plt.show()
### kNN多特征的决策边界
knn_clf_all = KNeighborsClassifier()
#knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)
plot_decision_boundary(knn_clf_all, axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1])
plt.show()

```

### 9-6 在逻辑回归中使用多项式特征

```python
import numpy as np
import matplotlib.pyplot as plt
np.random.seed = 666
X = np.random.normal(0,1,size=(200, 2))
y = np.array(X[:,0]**2 + X[:,1]**2 < 1.5,dtype='int')
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
###
#使用逻辑回归
%run D:\github\Mechanical-Learn\9\LogisticRegression.py
log_reg = LogisticRegression()
log_reg.fit(X,y)
log_reg.score(X, y)
###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[-4,4,-4,4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
###
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
%run D:\github\Mechanical-Learn\9\LogisticRegression.py
def PolynomailLogistRegression(degree):
    return Pipeline({
        ('poly',PolynomialFeatures(degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg',LogisticRegression())
    })
ploy_log_reg = PolynomailLogistRegression(degree=2)
ploy_log_reg.fit(X,y)
###
ploy_log_reg.score(X,y)
###
plot_decision_boundary(ploy_log_reg, axis=[-4,4,-4,4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()

```

### 9-7 scikit-learn 中的逻辑回归

```python
import numpy as np
import matplotlib.pyplot as plt
np.random.seed = 666
X = np.random.normal(0,1,size=(200, 2))
y = np.array(X[:,0]**2 + X[:,1] < 1.5,dtype='int')
for _ in range(20):
    y[np.random.randint(200)] = 1
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
###
#sklearn 逻辑回归
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
def PolynomailLogistRegression(degree,c,penalty='l2'):
    return Pipeline({
        ('poly',PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg',LogisticRegression(C=c, penalty='l2'))
    })
ploy_log_reg4 = PolynomailLogistRegression(degree=20, c=0.1, penalty='l1')
ploy_log_reg4.fit(x_train,y_train)
###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)


plot_decision_boundary(ploy_log_reg4, axis=[-4,4,-4,4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

### 9-8 OvR 与 OvO

```python
# OvR one vs Rest  ; OvO one vs one
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
def PolynomailLogistRegression(degree,c,penalty='l2'):
    return Pipeline({
        ('poly',PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg',LogisticRegression(C=c,penalty='l2'))
    })
# 默认 ovr
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
###
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)
plot_decision_boundary(log_reg, axis=[4,8.5,1.5,4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
#ovo
log_reg2 = LogisticRegression(multi_class='multinomial', solver='newton-cg')
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
###使用所有数据
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
iris = datasets.load_iris()

X = iris.data
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

from sklearn.linear_model import LogisticRegression
log_reg3 = LogisticRegression()
log_reg3.fit(X_train, y_train)
log_reg3.score(X_test, y_test)

###
log_reg4 = LogisticRegression(multi_class='multinomial', solver='newton-cg')
log_reg4.fit(X_train, y_train)
log_reg4.score(X_test, y_test)

### OvO OvR
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier
ovr = OneVsRestClassifier(log_reg3)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)
###
ovo = OneVsOneClassifier(log_reg3)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)


```
