## 第六章：梯度下降法

### 6-1 什么是梯度下降法

```python
#Gradient Descent
# 不是机器学习算法，基于搜索的最优化方法，
# 作用：最小化一个损失函数
# 梯度上升法：最大化一个效用函数

#导数：切线斜率  可以代表方向，对应J增大的方向
#学习率：影响速度
#全局最优解，
# 不是所有函数都有唯一的极值点：解决方案，多次运行，随机化初始点，初始点也是一个超参数
#线性回归的损失函数具有唯一的最优解
```

### 6-2 模拟实现梯度下降法

```python
#模拟实现
import numpy as np
import matplotlib.pyplot as plt

plot_x = np.linspace(-1,6 , 141)
plot_y = (plot_x-2.5)**2-1
plt.plot(plot_x, plot_y)
plt.show()


def dj(theta):
    return 2*(theta-2.5)
def J(theta):
    try:
        return (theta-2.5)**2-1
    except:
        return float('inf')

def gradient_descent(initial_theta, eta, n_inters = 1e4,epsilon=1e-8):
    theta = initial_theta
    theta_history.append(theta)
    i_inter=0
    while i_inter < n_inters:
        gradient = dj(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        i_inter += 1
        if(abs(J(theta) - J(last_theta)) < epsilon):
            break

def plot_theta_history():
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history),J(np.array(theta_history)),color='r', marker="+")
    plt.show()


eta = 1.1
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
len(theta_history)


```

### 6-3 线性回归中的梯度下降法

```python

#损失函数：J = Σ(y_true - y_predict)**2
# J(theta) = MSE(y_true , y_predict)

```

### 6-4 实现线性回归中的梯度下降法

```python
%run D:\github\Mechanical-Learn\6\LinearRegression.py
import numpy as np
import matplotlib.pyplot as plt
np.random.seed = 666
x = 2*np.random.random(size=100)
y = x*3. + 4. + np.random.normal(size=100)

X = x.reshape(-1,1)


lin_reg = LinearRegression()
lin_reg.fit_gd(X,y)
lin_reg.intercept_
lin_reg.coef_

```

### 6-5 梯度下降的向量化和数据标准化

```python
 #矩阵：(n+1)*m  *  m*1
# res = np.empty(len(theta))
# res[0] = np.sum(X_b.dot(theta) - y)
# for i in range(1, len(theta)):
#     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
# return res * 2 / len(X_b)

return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)
#例子
%run D:\github\Mechanical-Learn\6\LinearRegression.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets


boston = datasets.load_boston()
X = boston.data
y = boston.target

X= X[y<50.0]
y= y[y<50.0]

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)



gd_reg = LinearRegression()
#报warning eta过大
gd_reg.fit_gd(x_train,y_train)
#50s
%time gd_reg.fit_gd(x_train,y_train,eta=0.000001,n_inters=1e6)
#结果过大
gd_reg.score(x_test, y_test)

#使用梯度下降法前，最好进行数据归一化
from sklearn.preprocessing import StandardScaler
StandardScalers = StandardScaler()
StandardScalers.fit(x_train)
#数据归一化
X_train_standard = StandardScalers.transform(x_train)

%time gd_reg.fit_gd(X_train_standard,y_train)

#梯度下降法的优势  速度快


```

### 6-6 随机梯度下降法

```python
# Stochastic Grandient Descent
#模拟退火的思想， eta值：逐渐减少
#梯度下降法的优势  速度快
import numpy as np
import matplotlib.pyplot as plt

m = 10000
x = np.random.normal(size=m)
X = x.reshape(-1,1)
y = x*3. + 4. + np.random.normal(0,3,size=m)


def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(y)
    except:
        return float('inf')

# 计算梯度
def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

# 梯度下降法
def sgd(X_b, y, initial_theta, n_inters=1e4):
    t0 =5
    t1 = 50
    def learning_rate(t):
        return t0/(t+t1)

    theta = initial_theta
    i_inter = 0

    for cur_inter in range(n_inters):
        rand_i = np.random.randint(len(X_b))
        grandient = dJ_sgd(theta, X_b[rand_i],y[rand_i])
        theta = theta - learning_rate(cur_inter) * grandient
    return theta

X_b = np.hstack([np.ones((len(X),1)) , X])
initial_theta = np.zeros(X_b.shape[1])
theta = sgd(X_b, y , initial_theta, n_inters = len(X_b)//3)
theta
```

### 6-7 scikit-learn 中的随机梯度下降法

```python
import numpy as np
import matplotlib.pyplot as plt

m = 100000
x = np.random.normal(size=m)
X = x.reshape(-1,1)
y = x*3. + 4. + np.random.normal(0,3,size=m)


%run D:\github\Mechanical-Learn\6\LinearRegression.py

lin_reg = LinearRegression()
%time lin_reg.fit_sgd(X,y,n_inters=3)
lin_reg.coef_
lin_reg.intercept_


#随机梯度下降法
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets

#数据
boston = datasets.load_boston()
X = boston.data
y = boston.target

X= X[y<50.0]
y= y[y<50.0]

x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)
#数据归一化
from sklearn.preprocessing import StandardScaler
StandardScalers = StandardScaler()
StandardScalers.fit(x_train)

X_train_standard = StandardScalers.transform(x_train)
X_test_standard = StandardScalers.transform(x_test)

%run D:\github\Mechanical-Learn\6\LinearRegression.py
lin_reg = LinearRegression()
%time lin_reg.fit_sgd(X_train_standard,y_train,n_inters=5)
#模型结果
lin_reg.score(X_test_standard,y_test)


#scikit-learn 中的随机梯度下降法
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(n_iter=100)
%time sgd_reg.fit(X_train_standard,y_train)
sgd_reg.score(X_test_standard,y_test)

```

### 6-8 如何确定梯度计算的准确性 调试梯度下降法

```python
import numpy as np
import matplotlib.pyplot as plt
np.random.seed = 666
X = np.random.random(size=(1000,10))
true_theta = np.arange(1,12,dtype=float)
X_b = np.hstack([np.ones((len(X), 1)), X])
y = X_b.dot(true_theta) + np.random.normal(size=1000)

def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta))**2) / len(y)
            except:
                return float('inf')
def dJ_math(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)

def dJ_debug(theta, X_b, y, epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b , y) - J(theta_2, X_b , y)) / (2*epsilon)
    return res

def gradient_descent(dJ, X_b, y, initial_theta, eta, n_inters=1e3, epsilon=1e-8):
    theta = initial_theta
    i_inter = 0
    while i_inter < n_inters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        i_inter += 1
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
    return theta
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
#梯度求法调试  dJ_debug 机器学习工具箱  速度慢
%time gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
#梯度求法调试 dJ_math 具体求法  速度快
%time gradient_descent(dJ_math, X_b, y, initial_theta, eta)
```

### 6-9 有关梯度下降法的更多深入讨论

```python
# 随机梯度下降法  不稳定
# 批量梯度下降法  稳定
# 小批量梯度下降法 （结合手段）

#随机梯度下降法 （运算速度快，跳出局部最优解，）
```
