## 第十二章： 决策树

### 12-1 什么是决策树

```python
#Y N
#sklearn 中的决策树
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
datas = datasets.load_iris()
X = datas.data[:,2:]
y = datas.target

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
from sklearn.tree import DecisionTreeClassifier
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)


plot_decision_boundary(dt_clf, axis=[0.5,7.5,0,3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
# 非参数学习算法；解决分类，多分类，回归问题；非常好的可解析性
# 每个节点再哪个维度做划分
# 某个维度在哪个值上做划分


```

### 12-2 信息熵

```python
#12-2 信息熵：随机变量不确定度的度量，越小越好
# 1/3,1/3,1/3 ：熵比较大，  1，0，0 熵等于0，确定度最高

import numpy as np
import matplotlib.pyplot as plt


def entropy(p):
    return -p * np.log(p) - (1-p) * np.log(1-p)
#信息熵曲线
x = np.linspace(0.01, 0.99,200)
plt.plot(x, entropy(x))
plt.show()
#划分后信息熵降低
```

### 12-3 使用信息熵寻找最优划分

```python
#12-3
###划分
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
datas = datasets.load_iris()
X = datas.data[:,2:]
y = datas.target

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
from sklearn.tree import DecisionTreeClassifier
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(10,-1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(10,-1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict  = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    plt.contourf(x0, x1, zz,cmap=custom_cmap)


plot_decision_boundary(dt_clf, axis=[0.5,7.5,0,3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

###

#模拟使用信息熵进行划分
def split(X, y, d, value):
    index_a = (X[:,d] <= value)
    index_b = (X[:,d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

from collections import Counter
from math import log
def entropy(y):
    counter = Counter(y)
    res = 0.0;
    for num in counter.values():
        p = num / len(y)
        res += -p * log(p)
    return res

def try_split(X, y):
    best_entropy = float('inf')
    best_d , best_v = -1, -1
    for d in range(X.shape[1]):
        sorted_index = np.argsort(X[:,d])
        for i in range(1, len(X)):
            if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                x_l,x_r, y_l, y_r = split(X, y, d, v)
                e = entropy(y_l) + entropy(y_r)
                if e < best_entropy:
                    best_entropy,best_d,best_v= e, d, v
    return best_entropy,best_d,best_v
best_entropy,best_d,best_v = try_split(X,y)

x_l,x_r, y_l, y_r = split(X,y,best_d,best_v)
#信息熵是零
entropy(y_l)

###
entropy(y_r)
try_split(x_r, y_r)

```

### 12-4 基尼系数

```python
#模拟使用基尼系数

def split(X, y, d, value):
    index_a = (X[:,d] <= value)
    index_b = (X[:,d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

from collections import Counter
from math import log

def gini(y):
    counter = Counter(y)
    res = 1.0
    for num in counter.values():
        p = num / len(y)
        res -= p**2
    return res

def try_split(X, y):
    best_g = float('inf')
    best_d , best_v = -1, -1
    for d in range(X.shape[1]):
        sorted_index = np.argsort(X[:,d])
        for i in range(1, len(X)):
            if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                x_l,x_r, y_l, y_r = split(X, y, d, v)
                g = gini(y_l) + gini(y_r)
                if g < best_g:
                    best_g,best_d,best_v= g, d, v
    return best_g,best_d,best_v

best_g,best_d,best_v = try_split(X,y)

x_l,x_r, y_l, y_r = split(X,y,best_d,best_v)
###
gini(y_l)

###
gini(y_r)
try_split(x_r, y_r)

### 信息熵 计算慢  ， 基尼系数计算快
```

### 12-5 CART 与决策树中的超参数

```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
X, y = datasets.make_moons(noise=0.25, random_state=666)
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
###
from sklearn.tree import DecisionTreeClassifier
#过拟合
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X, y)
plot_decision_boundary(dt_clf, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
#参数  max_depth=2， min_samples_split=10，min_samples_leaf=4

dt_clf2 = DecisionTreeClassifier(max_depth=2)
dt_clf2.fit(X, y)
plot_decision_boundary(dt_clf2, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###

dt_clf3 = DecisionTreeClassifier(min_samples_split=10)
dt_clf3.fit(X, y)
plot_decision_boundary(dt_clf3, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
dt_clf4 = DecisionTreeClassifier(min_samples_leaf=4)
dt_clf4.fit(X, y)
plot_decision_boundary(dt_clf4, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()

```

### 12-6 决策树解决回归问题

```python
#12-6
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=666)

#Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
dt_reg = DecisionTreeRegressor()
dt_reg.fit(x_train,y_train)
###
dt_reg.score(x_test, y_test)

#训练数据过拟合 1
dt_reg.score(x_train, y_train)
```

### 12-7 决策树的局限性

```python
#12-7 决策树局限性
# 对个别数据敏感，高度依赖调参
import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
datas = datasets.load_iris()
X = datas.data[:,2:]
y = datas.target

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
from sklearn.tree import DecisionTreeClassifier
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5,7.5,0,3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
###
#
X_new = np.delete(X, 138, axis = 0)
y_new = np.delete(y, 138)

dt_clf2 = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf2.fit(X_new, y_new)

plot_decision_boundary(dt_clf2, axis=[0.5,7.5,0,3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
```
